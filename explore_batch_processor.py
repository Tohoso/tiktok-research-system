#!/usr/bin/env python3
"""
Explore Page Video Batch Processor
/exploreãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãŸå‹•ç”»URLã®è©³ç´°æƒ…å ±ã‚’ä¸€æ‹¬å–å¾—
"""

import os
import sys
import json
import time
import csv
from datetime import datetime
from typing import List, Dict, Any

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.logger import get_logger
from meta_tag_video_scraper import MetaTagVideoScraper


class ExploreBatchProcessor:
    """Exploreãƒšãƒ¼ã‚¸å‹•ç”»ã®ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)
        
        # APIã‚­ãƒ¼ã‚’å–å¾—
        self.api_key = os.getenv('SCRAPERAPI_KEY')
        if not self.api_key:
            raise ValueError("SCRAPERAPI_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ¡ã‚¿ã‚¿ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
        self.meta_scraper = MetaTagVideoScraper(self.api_key)
        
        # å‡¦ç†çµæœ
        self.results = {
            'start_time': datetime.now().isoformat(),
            'source': 'tiktok_explore_page',
            'processed_videos': [],
            'failed_videos': [],
            'summary': {}
        }
    
    def load_video_urls(self, filename: str = 'explore_video_urls.txt') -> List[str]:
        """å‹•ç”»URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        urls = []
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('https://www.tiktok.com/') and '/video/' in line:
                        urls.append(line)
            
            self.logger.info(f"å‹•ç”»URLèª­ã¿è¾¼ã¿å®Œäº†: {len(urls)}ä»¶")
            return urls
            
        except Exception as e:
            self.logger.error(f"å‹•ç”»URLèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def process_videos_batch(self, urls: List[str], max_videos: int = 30) -> Dict[str, Any]:
        """å‹•ç”»ã®ä¸€æ‹¬å‡¦ç†"""
        self.logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(urls)}ä»¶ã®å‹•ç”»")
        
        try:
            print(f"ğŸš€ /exploreãƒšãƒ¼ã‚¸å‹•ç”»ã®è©³ç´°æƒ…å ±å–å¾—é–‹å§‹")
            print(f"å¯¾è±¡å‹•ç”»æ•°: {min(len(urls), max_videos)}ä»¶")
            print("=" * 60)
            
            processed_count = 0
            failed_count = 0
            
            for i, url in enumerate(urls[:max_videos], 1):
                try:
                    print(f"\nğŸ“¹ å‹•ç”» {i}/{min(len(urls), max_videos)}: {url}")
                    print(f"   â³ è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­...")
                    
                    start_time = time.time()
                    details = self.meta_scraper.get_video_details(url)
                    end_time = time.time()
                    
                    processing_time = end_time - start_time
                    
                    if details:
                        # æˆåŠŸã—ãŸå ´åˆ
                        video_data = {
                            'url': url,
                            'video_id': details.get('video_id'),
                            'author_username': details.get('author_username'),
                            'like_count': details.get('like_count'),
                            'comment_count': details.get('comment_count'),
                            'title': details.get('og_title', ''),
                            'description': details.get('description', ''),
                            'keywords': details.get('keywords', ''),
                            'processing_time': processing_time,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.results['processed_videos'].append(video_data)
                        processed_count += 1
                        
                        print(f"   âœ… æˆåŠŸ ({processing_time:.1f}ç§’)")
                        print(f"   å‹•ç”»ID: {details.get('video_id')}")
                        print(f"   ä½œè€…: @{details.get('author_username')}")
                        if details.get('like_count'):
                            print(f"   ã„ã„ã­æ•°: {details.get('like_count'):,}")
                        if details.get('comment_count'):
                            print(f"   ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {details.get('comment_count'):,}")
                        
                    else:
                        # å¤±æ•—ã—ãŸå ´åˆ
                        failed_data = {
                            'url': url,
                            'error': 'è©³ç´°æƒ…å ±ã®å–å¾—ã«å¤±æ•—',
                            'processing_time': processing_time,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.results['failed_videos'].append(failed_data)
                        failed_count += 1
                        
                        print(f"   âŒ å¤±æ•—: è©³ç´°æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
                    
                    # é€²æ—è¡¨ç¤º
                    success_rate = processed_count / i * 100
                    print(f"   é€²æ—: {i}/{min(len(urls), max_videos)} ({success_rate:.1f}%æˆåŠŸ)")
                    
                    # APIåˆ¶é™ã‚’è€ƒæ…®ã—ãŸå¾…æ©Ÿ
                    if i < min(len(urls), max_videos):
                        print(f"   â³ æ¬¡ã®å‹•ç”»ã¾ã§å¾…æ©Ÿä¸­...")
                        time.sleep(2)  # 2ç§’å¾…æ©Ÿ
                    
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                    failed_data = {
                        'url': url,
                        'error': str(e),
                        'processing_time': 0,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    self.results['failed_videos'].append(failed_data)
                    failed_count += 1
                    
                    print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    self.logger.error(f"å‹•ç”»å‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            
            # ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
            self._generate_summary()
            
            # çµæœã‚’è¡¨ç¤º
            self._display_results()
            
            self.results['end_time'] = datetime.now().isoformat()
            self.logger.info(f"ãƒãƒƒãƒå‡¦ç†å®Œäº†: æˆåŠŸ{processed_count}ä»¶ã€å¤±æ•—{failed_count}ä»¶")
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.results['error'] = str(e)
            return self.results
    
    def _generate_summary(self):
        """å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        total_processed = len(self.results['processed_videos'])
        total_failed = len(self.results['failed_videos'])
        total_attempts = total_processed + total_failed
        
        self.results['summary'] = {
            'total_attempts': total_attempts,
            'successful_extractions': total_processed,
            'failed_extractions': total_failed,
            'success_rate': total_processed / total_attempts if total_attempts > 0 else 0,
            'average_processing_time': self._calculate_average_processing_time(),
            'unique_authors': len(set(video.get('author_username') for video in self.results['processed_videos'] if video.get('author_username'))),
            'total_likes': sum(video.get('like_count', 0) for video in self.results['processed_videos']),
            'total_comments': sum(video.get('comment_count', 0) for video in self.results['processed_videos'] if video.get('comment_count'))
        }
    
    def _calculate_average_processing_time(self) -> float:
        """å¹³å‡å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—"""
        processing_times = [video.get('processing_time', 0) for video in self.results['processed_videos']]
        return sum(processing_times) / len(processing_times) if processing_times else 0
    
    def _display_results(self):
        """çµæœã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š /exploreãƒšãƒ¼ã‚¸å‹•ç”»ãƒãƒƒãƒå‡¦ç†çµæœ")
        print("=" * 60)
        
        summary = self.results['summary']
        
        print(f"ç·å‡¦ç†æ•°: {summary['total_attempts']}")
        print(f"æˆåŠŸ: {summary['successful_extractions']}ä»¶")
        print(f"å¤±æ•—: {summary['failed_extractions']}ä»¶")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"å¹³å‡å‡¦ç†æ™‚é–“: {summary['average_processing_time']:.1f}ç§’")
        print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ä½œè€…æ•°: {summary['unique_authors']}")
        print(f"ç·ã„ã„ã­æ•°: {summary['total_likes']:,}")
        print(f"ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {summary['total_comments']:,}")
        
        if self.results['processed_videos']:
            print("\nğŸ† ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼:")
            sorted_videos = sorted(
                self.results['processed_videos'], 
                key=lambda x: x.get('like_count', 0), 
                reverse=True
            )
            for i, video in enumerate(sorted_videos[:5], 1):
                author = video.get('author_username', 'Unknown')
                likes = video.get('like_count', 0)
                print(f"{i}. @{author}: {likes:,}ã„ã„ã­")
    
    def save_results(self, filename: str = None) -> str:
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"explore_batch_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“„ è©³ç´°çµæœã‚’ä¿å­˜: {filename}")
            self.logger.info(f"çµæœä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def save_csv(self, filename: str = None) -> str:
        """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"explore_videos_{timestamp}.csv"
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'video_id', 'author_username', 'like_count', 'comment_count', 
                    'title', 'description', 'keywords', 'url', 'processing_time'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for video in self.results['processed_videos']:
                    writer.writerow({
                        'video_id': video.get('video_id', ''),
                        'author_username': video.get('author_username', ''),
                        'like_count': video.get('like_count', 0),
                        'comment_count': video.get('comment_count', 0),
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'keywords': video.get('keywords', ''),
                        'url': video.get('url', ''),
                        'processing_time': video.get('processing_time', 0)
                    })
            
            print(f"ğŸ“Š CSVå‡ºåŠ›å®Œäº†: {filename}")
            self.logger.info(f"CSVä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ TikTok /exploreãƒšãƒ¼ã‚¸å‹•ç”»ãƒãƒƒãƒå‡¦ç†")
    print("å–å¾—ã—ãŸå‹•ç”»URLã‹ã‚‰è©³ç´°æƒ…å ±ã‚’ä¸€æ‹¬å–å¾—ã—ã¾ã™")
    print("=" * 60)
    
    try:
        # ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
        processor = ExploreBatchProcessor()
        
        # å‹•ç”»URLã‚’èª­ã¿è¾¼ã¿
        urls = processor.load_video_urls()
        if not urls:
            print("âŒ å‹•ç”»URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        print(f"ğŸ“‹ èª­ã¿è¾¼ã¿å®Œäº†: {len(urls)}ä»¶ã®å‹•ç”»URL")
        
        # ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
        results = processor.process_videos_batch(urls, max_videos=30)
        
        # çµæœã‚’ä¿å­˜
        json_file = processor.save_results()
        csv_file = processor.save_csv()
        
        # æœ€çµ‚è©•ä¾¡
        summary = results['summary']
        success_rate = summary['success_rate']
        
        print(f"\nğŸ¯ æœ€çµ‚è©•ä¾¡:")
        if success_rate >= 0.8:
            print("ğŸŸ¢ å„ªç§€ - é«˜ã„æˆåŠŸç‡ã§å‡¦ç†å®Œäº†")
        elif success_rate >= 0.6:
            print("ğŸŸ¡ è‰¯å¥½ - æ¦‚ã­æˆåŠŸ")
        else:
            print("ğŸŸ  è¦æ”¹å–„ - æˆåŠŸç‡ãŒä½ã„")
        
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        print(f"å‡¦ç†å®Œäº†: {summary['successful_extractions']}/{summary['total_attempts']}ä»¶")
        
        if json_file:
            print(f"è©³ç´°çµæœ: {json_file}")
        if csv_file:
            print(f"CSVå‡ºåŠ›: {csv_file}")
        
        return success_rate >= 0.5
        
    except Exception as e:
        print(f"âŒ ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


#!/usr/bin/env python3
"""
Meta Tag Based Video Detail Scraper for TikTok Research System
ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import os
import sys
import re
import json
import time
import random
from typing import Dict, Any, Optional, List
from datetime import datetime
from bs4 import BeautifulSoup

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.scraper.scraperapi_client import ScraperAPIClient
from src.utils.logger import get_logger
from src.parser.video_data import VideoData


class MetaTagVideoScraper:
    """ãƒ¡ã‚¿ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, api_key: str):
        self.logger = get_logger(self.__class__.__name__)
        self.api_client = ScraperAPIClient(api_key)
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'videos_with_details': 0,
            'meta_tag_extractions': 0,
            'view_count_extracted': 0,
            'like_count_extracted': 0,
            'comment_count_extracted': 0,
            'author_extracted': 0
        }
    
    def get_video_details(self, video_url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        å€‹åˆ¥å‹•ç”»ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ¡ã‚¿ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ï¼‰
        
        Args:
            video_url: å‹•ç”»URL
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            å‹•ç”»è©³ç´°æƒ…å ±ã®è¾æ›¸ã€å¤±æ•—æ™‚ã¯None
        """
        self.logger.info(f"å‹•ç”»è©³ç´°æƒ…å ±ã‚’å–å¾—: {video_url}")
        
        for attempt in range(max_retries):
            try:
                self.stats['total_requests'] += 1
                
                # JavaScriptå®Ÿè¡Œã‚ã‚Šã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                response = self.api_client.scrape(
                    url=video_url,
                    render_js=True,
                    country_code='JP',
                    premium=True
                )
                
                if response and response.get('status_code') == 200:
                    html_content = response.get('content', '')
                    
                    if html_content and len(html_content) > 1000:
                        details = self._extract_video_details_from_meta(html_content, video_url)
                        
                        if details:
                            self.stats['successful_requests'] += 1
                            self.stats['videos_with_details'] += 1
                            self.logger.info(f"è©³ç´°æƒ…å ±å–å¾—æˆåŠŸ: {video_url}")
                            return details
                        else:
                            self.logger.warning(f"è©³ç´°æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—: {video_url}")
                    else:
                        self.logger.warning(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸ååˆ†: {video_url} (ã‚µã‚¤ã‚º: {len(html_content)})")
                else:
                    self.logger.warning(f"HTTP ã‚¨ãƒ©ãƒ¼: {response.get('status_code')} - {video_url}")
                
                # ãƒªãƒˆãƒ©ã‚¤å‰ã®å¾…æ©Ÿ
                if attempt < max_retries - 1:
                    wait_time = random.uniform(5, 15)
                    self.logger.info(f"ãƒªãƒˆãƒ©ã‚¤å‰å¾…æ©Ÿ: {wait_time:.1f}ç§’")
                    time.sleep(wait_time)
                
            except Exception as e:
                self.logger.error(f"å‹•ç”»è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    wait_time = random.uniform(10, 20)
                    time.sleep(wait_time)
        
        self.stats['failed_requests'] += 1
        self.logger.error(f"å‹•ç”»è©³ç´°å–å¾—å¤±æ•—: {video_url}")
        return None
    
    def _extract_video_details_from_meta(self, html_content: str, video_url: str) -> Optional[Dict[str, Any]]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰å‹•ç”»è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            video_url: å‹•ç”»URL
            
        Returns:
            å‹•ç”»è©³ç´°æƒ…å ±ã®è¾æ›¸
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            details = {}
            
            # å‹•ç”»IDã‚’æŠ½å‡º
            video_id_match = re.search(r'/video/(\d+)', video_url)
            if video_id_match:
                details['video_id'] = video_id_match.group(1)
            
            # ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
            meta_info = self._extract_meta_tags(soup)
            details.update(meta_info)
            
            # descriptionãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
            stats_info = self._extract_stats_from_description(soup)
            details.update(stats_info)
            
            # keywordsãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º
            keywords_info = self._extract_keywords(soup)
            details.update(keywords_info)
            
            # JSON-LDã‹ã‚‰è£œå®Œæƒ…å ±ã‚’æŠ½å‡º
            json_ld_info = self._extract_json_ld(soup)
            details.update(json_ld_info)
            
            # åŸºæœ¬æƒ…å ±ã®è£œå®Œ
            details['url'] = video_url
            details['scraped_at'] = datetime.now().isoformat()
            details['extraction_method'] = 'meta_tag_based'
            
            # çµ±è¨ˆæ›´æ–°
            if details.get('like_count'):
                self.stats['like_count_extracted'] += 1
            if details.get('comment_count'):
                self.stats['comment_count_extracted'] += 1
            if details.get('view_count'):
                self.stats['view_count_extracted'] += 1
            if details.get('author_username'):
                self.stats['author_extracted'] += 1
            
            self.stats['meta_tag_extractions'] += 1
            
            # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯
            has_details = any([
                details.get('like_count'),
                details.get('comment_count'),
                details.get('title'),
                details.get('author_username')
            ])
            
            if has_details:
                self.logger.debug(f"æŠ½å‡ºã•ã‚ŒãŸè©³ç´°æƒ…å ±: {details}")
                return details
            else:
                self.logger.warning("æœ‰åŠ¹ãªè©³ç´°æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
                
        except Exception as e:
            self.logger.error(f"è©³ç´°æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_meta_tags(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        details = {}
        
        try:
            # Open Graphãƒ¡ã‚¿ã‚¿ã‚°
            og_tags = {
                'og:title': 'og_title',
                'og:description': 'og_description',
                'og:image': 'thumbnail_url',
                'og:url': 'canonical_url'
            }
            
            for og_property, detail_key in og_tags.items():
                meta_tag = soup.find('meta', property=og_property)
                if meta_tag and meta_tag.get('content'):
                    details[detail_key] = meta_tag['content']
            
            # Twitterã‚«ãƒ¼ãƒ‰ãƒ¡ã‚¿ã‚¿ã‚°
            twitter_tags = {
                'twitter:title': 'twitter_title',
                'twitter:description': 'twitter_description',
                'twitter:image': 'twitter_image'
            }
            
            for twitter_name, detail_key in twitter_tags.items():
                meta_tag = soup.find('meta', attrs={'name': twitter_name})
                if meta_tag and meta_tag.get('content'):
                    details[detail_key] = meta_tag['content']
            
            # åŸºæœ¬ãƒ¡ã‚¿ã‚¿ã‚°
            basic_tags = {
                'description': 'meta_description',
                'keywords': 'meta_keywords'
            }
            
            for meta_name, detail_key in basic_tags.items():
                meta_tag = soup.find('meta', attrs={'name': meta_name})
                if meta_tag and meta_tag.get('content'):
                    details[detail_key] = meta_tag['content']
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°
            title_tag = soup.find('title')
            if title_tag and title_tag.string:
                details['page_title'] = title_tag.string.strip()
        
        except Exception as e:
            self.logger.warning(f"ãƒ¡ã‚¿ã‚¿ã‚°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_stats_from_description(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """descriptionãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º"""
        details = {}
        
        try:
            # descriptionãƒ¡ã‚¿ã‚¿ã‚°ã‚’å–å¾—
            desc_tag = soup.find('meta', attrs={'name': 'description'})
            if not desc_tag or not desc_tag.get('content'):
                return details
            
            description = desc_tag['content']
            self.logger.debug(f"Descriptionå†…å®¹: {description}")
            
            # æ—¥æœ¬èªã®çµ±è¨ˆæƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns = {
                'like_count': [
                    r'ã„ã„ã­ã®æ•°[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'ã„ã„ã­[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'â™¥\s*(\d+(?:\.\d+)?[KMB]?)'
                ],
                'comment_count': [
                    r'ã‚³ãƒ¡ãƒ³ãƒˆã®æ•°[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'ã‚³ãƒ¡ãƒ³ãƒˆ[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)'
                ],
                'view_count': [
                    r'å†ç”Ÿå›æ•°[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'å†ç”Ÿ[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'(\d+(?:\.\d+)?[KMB]?)\s*å›å†ç”Ÿ'
                ],
                'share_count': [
                    r'ã‚·ã‚§ã‚¢[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                    r'å…±æœ‰[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)'
                ]
            }
            
            # å„çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
            for stat_type, pattern_list in patterns.items():
                for pattern in pattern_list:
                    match = re.search(pattern, description, re.IGNORECASE)
                    if match:
                        count_str = match.group(1)
                        count = self._parse_count_string(count_str)
                        if count is not None and count > 0:
                            details[stat_type] = count
                            self.logger.debug(f"{stat_type}ã‚’æŠ½å‡º: {count_str} -> {count}")
                            break
            
            # ä½œè€…æƒ…å ±ã‚’æŠ½å‡º
            author_patterns = [
                r'([^(]+)\s*\(@([^)]+)\)',  # "Destined DestinÃ© (@_quietlydope)"
                r'@([a-zA-Z0-9_]+)',       # "@_quietlydope"
            ]
            
            for pattern in author_patterns:
                match = re.search(pattern, description)
                if match:
                    if len(match.groups()) == 2:
                        details['author_display_name'] = match.group(1).strip()
                        details['author_username'] = match.group(2).strip()
                    else:
                        details['author_username'] = match.group(1).strip()
                    break
            
            # å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«/èª¬æ˜ã‚’æŠ½å‡º
            title_patterns = [
                r'å‹•ç”»[ï¼š:]ã€Œ([^ã€]+)ã€',
                r'TikTok[^ï¼š:]*[ï¼š:]ã€Œ([^ã€]+)ã€',
            ]
            
            for pattern in title_patterns:
                match = re.search(pattern, description)
                if match:
                    details['video_title'] = match.group(1).strip()
                    break
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            hashtag_match = re.search(r'ã€‚([^ã€‚]+)ã€‚', description)
            if hashtag_match:
                details['hashtags'] = hashtag_match.group(1).strip()
        
        except Exception as e:
            self.logger.warning(f"çµ±è¨ˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_keywords(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """keywordsãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        details = {}
        
        try:
            keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
            if keywords_tag and keywords_tag.get('content'):
                keywords = keywords_tag['content']
                details['keywords'] = [k.strip() for k in keywords.split(',') if k.strip()]
                self.logger.debug(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º: {details['keywords']}")
        
        except Exception as e:
            self.logger.warning(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_json_ld(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è£œå®Œæƒ…å ±ã‚’æŠ½å‡º"""
        details = {}
        
        try:
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            
            for script in json_ld_scripts:
                if script.string:
                    try:
                        data = json.loads(script.string)
                        
                        if isinstance(data, dict) and data.get('@type') == 'VideoObject':
                            if 'name' in data:
                                details['json_ld_title'] = data['name']
                            if 'description' in data:
                                details['json_ld_description'] = data['description']
                            if 'uploadDate' in data:
                                details['upload_date'] = data['uploadDate']
                            if 'duration' in data:
                                details['duration'] = data['duration']
                            
                            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ
                            if 'interactionStatistic' in data:
                                for stat in data['interactionStatistic']:
                                    interaction_type = stat.get('interactionType', {}).get('@type', '')
                                    count = stat.get('userInteractionCount')
                                    
                                    if interaction_type == 'LikeAction' and count:
                                        details['json_ld_like_count'] = int(count)
                                    elif interaction_type == 'CommentAction' and count:
                                        details['json_ld_comment_count'] = int(count)
                                    elif interaction_type == 'ShareAction' and count:
                                        details['json_ld_share_count'] = int(count)
                                    elif interaction_type == 'WatchAction' and count:
                                        details['json_ld_view_count'] = int(count)
                        
                    except json.JSONDecodeError:
                        continue
        
        except Exception as e:
            self.logger.warning(f"JSON-LDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_count_string(self, count_str: str) -> Optional[int]:
        """ã‚«ã‚¦ãƒ³ãƒˆæ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›"""
        try:
            count_str = count_str.upper().strip().replace(',', '').replace(' ', '')
            
            if count_str.endswith('K'):
                return int(float(count_str[:-1]) * 1000)
            elif count_str.endswith('M'):
                return int(float(count_str[:-1]) * 1000000)
            elif count_str.endswith('B'):
                return int(float(count_str[:-1]) * 1000000000)
            else:
                return int(float(count_str))
                
        except (ValueError, TypeError):
            return None
    
    def get_multiple_video_details(
        self,
        video_urls: List[str],
        delay_between_requests: float = 3.0
    ) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã®å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            video_urls: å‹•ç”»URLã®ãƒªã‚¹ãƒˆ
            delay_between_requests: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“
            
        Returns:
            å‹•ç”»è©³ç´°æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        for i, url in enumerate(video_urls):
            self.logger.info(f"å‹•ç”» {i+1}/{len(video_urls)} ã‚’å‡¦ç†ä¸­: {url}")
            
            details = self.get_video_details(url)
            if details:
                results.append(details)
            
            # æœ€å¾Œä»¥å¤–ã¯å¾…æ©Ÿ
            if i < len(video_urls) - 1:
                time.sleep(delay_between_requests)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            **self.stats,
            'success_rate': self.stats['successful_requests'] / max(self.stats['total_requests'], 1),
            'like_count_extraction_rate': self.stats['like_count_extracted'] / max(self.stats['videos_with_details'], 1),
            'comment_count_extraction_rate': self.stats['comment_count_extracted'] / max(self.stats['videos_with_details'], 1),
            'author_extraction_rate': self.stats['author_extracted'] / max(self.stats['videos_with_details'], 1)
        }


def test_meta_tag_scraper():
    """ãƒ¡ã‚¿ã‚¿ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # APIã‚­ãƒ¼ã‚’å–å¾—
    api_key = os.getenv('SCRAPERAPI_KEY')
    if not api_key:
        print("âŒ SCRAPERAPI_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # ãƒ†ã‚¹ãƒˆç”¨å‹•ç”»URL
    test_urls = [
        "https://www.tiktok.com/@_quietlydope/video/7535094688726945079",
        "https://www.tiktok.com/@ohnoitsrolo/video/7534370912854985997",
    ]
    
    scraper = MetaTagVideoScraper(api_key)
    
    print("ğŸ” ãƒ¡ã‚¿ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹å‹•ç”»è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n=== ãƒ†ã‚¹ãƒˆ {i}: {url} ===")
        
        details = scraper.get_video_details(url)
        
        if details:
            print("âœ… è©³ç´°æƒ…å ±å–å¾—æˆåŠŸ")
            print(f"å‹•ç”»ID: {details.get('video_id', 'N/A')}")
            print(f"ä½œè€…: {details.get('author_username', 'N/A')} ({details.get('author_display_name', 'N/A')})")
            print(f"ã„ã„ã­æ•°: {details.get('like_count', 'N/A')}")
            print(f"ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {details.get('comment_count', 'N/A')}")
            print(f"å†ç”Ÿæ•°: {details.get('view_count', 'N/A')}")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {details.get('video_title', details.get('og_title', 'N/A'))}")
            print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {details.get('keywords', 'N/A')}")
            
            # è©³ç´°æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            os.makedirs('debug', exist_ok=True)
            with open(f'debug/meta_tag_video_details_{details.get("video_id", i)}.json', 'w', encoding='utf-8') as f:
                json.dump(details, f, ensure_ascii=False, indent=2)
            print(f"è©³ç´°æƒ…å ±ã‚’ä¿å­˜: debug/meta_tag_video_details_{details.get('video_id', i)}.json")
        else:
            print("âŒ è©³ç´°æƒ…å ±å–å¾—å¤±æ•—")
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    stats = scraper.get_stats()
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats['total_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"ã„ã„ã­æ•°æŠ½å‡ºç‡: {stats['like_count_extraction_rate']:.2%}")
    print(f"ã‚³ãƒ¡ãƒ³ãƒˆæ•°æŠ½å‡ºç‡: {stats['comment_count_extraction_rate']:.2%}")
    print(f"ä½œè€…æƒ…å ±æŠ½å‡ºç‡: {stats['author_extraction_rate']:.2%}")


if __name__ == "__main__":
    test_meta_tag_scraper()


#!/usr/bin/env python3
"""
Enhanced Video Detail Scraper for TikTok Research System
å†ç”Ÿæ•°ã€æŠ•ç¨¿æ—¥æ™‚ã€ä½œè€…æƒ…å ±ã®æŠ½å‡ºã‚’å¼·åŒ–
"""

import re
import json
import time
import random
from typing import Dict, Any, Optional, List
from datetime import datetime
from bs4 import BeautifulSoup

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.logger import get_logger
from src.utils.user_agents import UserAgentManager
from src.utils.proxy_manager import ProxyManager
from src.utils.request_throttle import RequestThrottle
from src.scraper.scraperapi_client import ScraperAPIClient
from src.parser.video_data import VideoData


class EnhancedVideoDetailScraper:
    """æ”¹è‰¯ç‰ˆå€‹åˆ¥å‹•ç”»è©³ç´°æƒ…å ±å–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str):
        self.logger = get_logger(self.__class__.__name__)
        self.api_client = ScraperAPIClient(api_key)
        self.proxy_manager = ProxyManager()
        self.throttle = RequestThrottle()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'videos_with_details': 0,
            'videos_without_details': 0,
            'view_count_extracted': 0,
            'create_time_extracted': 0,
            'author_extracted': 0
        }
    
    def get_video_details(self, video_url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        å€‹åˆ¥å‹•ç”»ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            video_url: å‹•ç”»URL
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            å‹•ç”»è©³ç´°æƒ…å ±ã®è¾æ›¸ã€å¤±æ•—æ™‚ã¯None
        """
        self.logger.info(f"å‹•ç”»è©³ç´°æƒ…å ±ã‚’å–å¾—: {video_url}")
        
        for attempt in range(max_retries):
            try:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶å¾¡
                self.throttle.wait_if_needed()
                
                # User-Agentã¨ãƒ—ãƒ­ã‚­ã‚·ã‚’é¸æŠ
                user_agent = UserAgentManager.get_random_tiktok_agent()
                proxy = self.proxy_manager.get_next_proxy()
                
                # ScraperAPIã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                custom_params = {
                    'premium': True,
                    'session_number': random.randint(1, 1000),
                    'keep_headers': True,
                    'render_js': True  # JavaScriptå®Ÿè¡Œã‚’æœ‰åŠ¹åŒ–
                }
                
                if proxy and hasattr(proxy, 'country') and proxy.country:
                    custom_params['country_code'] = proxy.country
                
                self.stats['total_requests'] += 1
                
                response = self.api_client.scrape(
                    url=video_url,
                    **custom_params
                )
                
                if response and response.get('status_code') == 200:
                    html_content = response.get('content', '')
                    
                    if html_content and len(html_content) > 1000:  # æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                        details = self._extract_video_details_enhanced(html_content, video_url)
                        
                        if details:
                            self.stats['successful_requests'] += 1
                            self.stats['videos_with_details'] += 1
                            
                            # æŠ½å‡ºæˆåŠŸçµ±è¨ˆ
                            if details.get('view_count'):
                                self.stats['view_count_extracted'] += 1
                            if details.get('create_time'):
                                self.stats['create_time_extracted'] += 1
                            if details.get('author_username'):
                                self.stats['author_extracted'] += 1
                            
                            self.logger.info(f"è©³ç´°æƒ…å ±å–å¾—æˆåŠŸ: {video_url}")
                            return details
                        else:
                            self.stats['videos_without_details'] += 1
                            self.logger.warning(f"è©³ç´°æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—: {video_url}")
                    else:
                        self.logger.warning(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸ååˆ†: {video_url} (ã‚µã‚¤ã‚º: {len(html_content)})")
                else:
                    self.logger.warning(f"HTTP ã‚¨ãƒ©ãƒ¼: {response.get('status_code')} - {video_url}")
                
                # ãƒªãƒˆãƒ©ã‚¤å‰ã®å¾…æ©Ÿ
                if attempt < max_retries - 1:
                    wait_time = random.uniform(5, 15)
                    self.logger.info(f"ãƒªãƒˆãƒ©ã‚¤å‰å¾…æ©Ÿ: {wait_time:.1f}ç§’")
                    time.sleep(wait_time)
                
            except Exception as e:
                self.logger.error(f"å‹•ç”»è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    wait_time = random.uniform(10, 20)
                    time.sleep(wait_time)
        
        self.stats['failed_requests'] += 1
        self.logger.error(f"å‹•ç”»è©³ç´°å–å¾—å¤±æ•—: {video_url}")
        return None
    
    def _extract_video_details_enhanced(self, html_content: str, video_url: str) -> Optional[Dict[str, Any]]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰å‹•ç”»è©³ç´°æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            html_content: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            video_url: å‹•ç”»URL
            
        Returns:
            å‹•ç”»è©³ç´°æƒ…å ±ã®è¾æ›¸
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            details = {}
            
            # å‹•ç”»IDã‚’æŠ½å‡º
            video_id_match = re.search(r'/video/(\d+)', video_url)
            if video_id_match:
                details['video_id'] = video_id_match.group(1)
            
            # æ–¹æ³•1: SIGI_STATEï¼ˆæœ€å„ªå…ˆï¼‰
            sigi_data = self._extract_from_sigi_state_enhanced(html_content)
            if sigi_data:
                details.update(sigi_data)
                self.logger.debug(f"SIGI_STATEæŠ½å‡ºæˆåŠŸ: {len(sigi_data)}é …ç›®")
            
            # æ–¹æ³•2: __UNIVERSAL_DATA_FOR_REHYDRATION__
            universal_data = self._extract_from_universal_data(html_content)
            if universal_data:
                details.update(universal_data)
                self.logger.debug(f"Universal DataæŠ½å‡ºæˆåŠŸ: {len(universal_data)}é …ç›®")
            
            # æ–¹æ³•3: JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
            json_ld_data = self._extract_from_json_ld_enhanced(soup)
            if json_ld_data:
                details.update(json_ld_data)
                self.logger.debug(f"JSON-LDæŠ½å‡ºæˆåŠŸ: {len(json_ld_data)}é …ç›®")
            
            # æ–¹æ³•4: ãƒ¡ã‚¿ã‚¿ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            meta_data = self._extract_from_meta_tags_enhanced(soup)
            if meta_data:
                details.update(meta_data)
                self.logger.debug(f"ãƒ¡ã‚¿ã‚¿ã‚°æŠ½å‡ºæˆåŠŸ: {len(meta_data)}é …ç›®")
            
            # æ–¹æ³•5: HTMLãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            text_data = self._extract_from_text_content_enhanced(soup)
            if text_data:
                details.update(text_data)
                self.logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæˆåŠŸ: {len(text_data)}é …ç›®")
            
            # æ–¹æ³•6: data-e2eå±æ€§ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            e2e_data = self._extract_from_e2e_attributes_enhanced(soup)
            if e2e_data:
                details.update(e2e_data)
                self.logger.debug(f"data-e2eæŠ½å‡ºæˆåŠŸ: {len(e2e_data)}é …ç›®")
            
            # æ–¹æ³•7: CSS ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹ç›´æ¥æŠ½å‡º
            css_data = self._extract_from_css_selectors(soup)
            if css_data:
                details.update(css_data)
                self.logger.debug(f"CSS ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼æŠ½å‡ºæˆåŠŸ: {len(css_data)}é …ç›®")
            
            # æ–¹æ³•8: æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹é«˜åº¦ãªæŠ½å‡º
            regex_data = self._extract_from_regex_patterns(html_content)
            if regex_data:
                details.update(regex_data)
                self.logger.debug(f"æ­£è¦è¡¨ç¾æŠ½å‡ºæˆåŠŸ: {len(regex_data)}é …ç›®")
            
            # åŸºæœ¬æƒ…å ±ã®è£œå®Œ
            details['url'] = video_url
            details['scraped_at'] = datetime.now().isoformat()
            
            # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããŸã‹ãƒã‚§ãƒƒã‚¯
            has_details = any([
                details.get('view_count'),
                details.get('like_count'),
                details.get('create_time'),
                details.get('author_username')
            ])
            
            if has_details:
                self.logger.debug(f"æŠ½å‡ºã•ã‚ŒãŸè©³ç´°æƒ…å ±: {details}")
                return details
            else:
                self.logger.warning("æœ‰åŠ¹ãªè©³ç´°æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
                
        except Exception as e:
            self.logger.error(f"è©³ç´°æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_from_sigi_state_enhanced(self, html_content: str) -> Dict[str, Any]:
        """SIGI_STATEï¼ˆTikTokã®å†…éƒ¨çŠ¶æ…‹ï¼‰ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            # è¤‡æ•°ã®SIGI_STATEãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            sigi_patterns = [
                r'window\[\'SIGI_STATE\'\]\s*=\s*({.+?});',
                r'window\.SIGI_STATE\s*=\s*({.+?});',
                r'SIGI_STATE\s*=\s*({.+?});',
                r'"SIGI_STATE":\s*({.+?})',
            ]
            
            for pattern in sigi_patterns:
                sigi_match = re.search(pattern, html_content, re.DOTALL)
                
                if sigi_match:
                    try:
                        sigi_data = json.loads(sigi_match.group(1))
                        
                        # ItemModuleã‹ã‚‰å‹•ç”»æƒ…å ±ã‚’æŠ½å‡º
                        if 'ItemModule' in sigi_data:
                            item_module = sigi_data['ItemModule']
                            for video_id, video_data in item_module.items():
                                if isinstance(video_data, dict):
                                    extracted = self._parse_item_module_data_enhanced(video_data)
                                    details.update(extracted)
                                    if extracted:  # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸã‚‰æœ€åˆã®ã‚‚ã®ã‚’ä½¿ç”¨
                                        break
                        
                        # UserModuleã‹ã‚‰ä½œè€…æƒ…å ±ã‚’æŠ½å‡º
                        if 'UserModule' in sigi_data:
                            user_module = sigi_data['UserModule']
                            if 'users' in user_module:
                                for user_id, user_data in user_module['users'].items():
                                    if isinstance(user_data, dict):
                                        extracted = self._parse_user_module_data_enhanced(user_data)
                                        details.update(extracted)
                                        if extracted:  # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸã‚‰æœ€åˆã®ã‚‚ã®ã‚’ä½¿ç”¨
                                            break
                        
                        # VideoDetailã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
                        if 'VideoDetail' in sigi_data:
                            video_detail = sigi_data['VideoDetail']
                            extracted = self._parse_video_detail_data(video_detail)
                            details.update(extracted)
                        
                        if details:  # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†
                            break
                    
                    except json.JSONDecodeError as e:
                        self.logger.debug(f"SIGI_STATE JSONè§£æã‚¨ãƒ©ãƒ¼ (ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern}): {e}")
                        continue
        
        except Exception as e:
            self.logger.warning(f"SIGI_STATEæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_universal_data(self, html_content: str) -> Dict[str, Any]:
        """__UNIVERSAL_DATA_FOR_REHYDRATION__ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        details = {}
        
        try:
            # Universal Dataãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            universal_patterns = [
                r'window\[\'__UNIVERSAL_DATA_FOR_REHYDRATION__\'\]\s*=\s*({.+?});',
                r'__UNIVERSAL_DATA_FOR_REHYDRATION__\s*=\s*({.+?});',
            ]
            
            for pattern in universal_patterns:
                universal_match = re.search(pattern, html_content, re.DOTALL)
                
                if universal_match:
                    try:
                        universal_data = json.loads(universal_match.group(1))
                        
                        # __DEFAULT_SCOPE__ã‹ã‚‰å‹•ç”»æƒ…å ±ã‚’æŠ½å‡º
                        if '__DEFAULT_SCOPE__' in universal_data:
                            default_scope = universal_data['__DEFAULT_SCOPE__']
                            
                            # webapp.video-detailã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
                            if 'webapp.video-detail' in default_scope:
                                video_detail = default_scope['webapp.video-detail']
                                extracted = self._parse_webapp_video_detail(video_detail)
                                details.update(extracted)
                        
                        if details:  # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†
                            break
                    
                    except json.JSONDecodeError as e:
                        self.logger.debug(f"Universal Data JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                        continue
        
        except Exception as e:
            self.logger.warning(f"Universal DataæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_json_ld_enhanced(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            
            for script in json_ld_scripts:
                if script.string:
                    try:
                        data = json.loads(script.string)
                        
                        if isinstance(data, dict):
                            # VideoObjectã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
                            if data.get('@type') == 'VideoObject':
                                extracted = self._parse_video_object_enhanced(data)
                                details.update(extracted)
                            
                            # ãã®ä»–ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
                            if 'interactionStatistic' in data:
                                extracted = self._parse_interaction_stats_enhanced(data['interactionStatistic'])
                                details.update(extracted)
                        
                        elif isinstance(data, list):
                            # é…åˆ—ã®å ´åˆã¯å„è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
                            for item in data:
                                if isinstance(item, dict) and item.get('@type') == 'VideoObject':
                                    extracted = self._parse_video_object_enhanced(item)
                                    details.update(extracted)
                        
                    except json.JSONDecodeError:
                        continue
        
        except Exception as e:
            self.logger.warning(f"JSON-LDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_meta_tags_enhanced(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            # Open Graphãƒ¡ã‚¿ã‚¿ã‚°
            og_tags = {
                'og:title': 'title',
                'og:description': 'description',
                'og:image': 'thumbnail_url',
                'og:video': 'video_url',
                'og:video:duration': 'duration',
                'og:video:width': 'width',
                'og:video:height': 'height'
            }
            
            for og_property, detail_key in og_tags.items():
                meta_tag = soup.find('meta', property=og_property)
                if meta_tag and meta_tag.get('content'):
                    content = meta_tag['content']
                    if detail_key in ['duration', 'width', 'height']:
                        try:
                            details[detail_key] = int(content)
                        except ValueError:
                            pass
                    else:
                        details[detail_key] = content
            
            # Twitterã‚«ãƒ¼ãƒ‰ãƒ¡ã‚¿ã‚¿ã‚°
            twitter_tags = {
                'twitter:title': 'title',
                'twitter:description': 'description',
                'twitter:image': 'thumbnail_url',
                'twitter:player:width': 'width',
                'twitter:player:height': 'height'
            }
            
            for twitter_name, detail_key in twitter_tags.items():
                meta_tag = soup.find('meta', attrs={'name': twitter_name})
                if meta_tag and meta_tag.get('content'):
                    if detail_key not in details:  # OGã‚¿ã‚°ã‚’å„ªå…ˆ
                        content = meta_tag['content']
                        if detail_key in ['width', 'height']:
                            try:
                                details[detail_key] = int(content)
                            except ValueError:
                                pass
                        else:
                            details[detail_key] = content
            
            # TikTokç‰¹æœ‰ã®ãƒ¡ã‚¿ã‚¿ã‚°
            tiktok_meta_tags = {
                'tiktok:video:id': 'video_id',
                'tiktok:author': 'author_username',
                'tiktok:upload_date': 'create_time'
            }
            
            for meta_name, detail_key in tiktok_meta_tags.items():
                meta_tag = soup.find('meta', attrs={'name': meta_name})
                if meta_tag and meta_tag.get('content'):
                    details[detail_key] = meta_tag['content']
        
        except Exception as e:
            self.logger.warning(f"ãƒ¡ã‚¿ã‚¿ã‚°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_text_content_enhanced(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """HTMLãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            text_content = soup.get_text()
            
            # å†ç”Ÿæ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            view_patterns = [
                r'(\d+(?:\.\d+)?[KMB]?)\s*(?:views?|å†ç”Ÿ|å›å†ç”Ÿ|æ¬¡å†ç”Ÿ)',
                r'(\d+(?:,\d+)*)\s*(?:views?|å†ç”Ÿ)',
                r'å†ç”Ÿå›æ•°[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                r'view[s]?\s*[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
            ]
            
            for pattern in view_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    view_count = self._parse_count_string(match.group(1))
                    if view_count and view_count > 0:
                        details['view_count'] = view_count
                        break
            
            # ã„ã„ã­æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            like_patterns = [
                r'(\d+(?:\.\d+)?[KMB]?)\s*(?:likes?|ã„ã„ã­|â™¥)',
                r'â™¥\s*(\d+(?:\.\d+)?[KMB]?)',
                r'ã„ã„ã­[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                r'like[s]?\s*[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
            ]
            
            for pattern in like_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    like_count = self._parse_count_string(match.group(1))
                    if like_count and like_count > 0:
                        details['like_count'] = like_count
                        break
            
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            comment_patterns = [
                r'(\d+(?:\.\d+)?[KMB]?)\s*(?:comments?|ã‚³ãƒ¡ãƒ³ãƒˆ)',
                r'ã‚³ãƒ¡ãƒ³ãƒˆ[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                r'comment[s]?\s*[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
            ]
            
            for pattern in comment_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    comment_count = self._parse_count_string(match.group(1))
                    if comment_count and comment_count > 0:
                        details['comment_count'] = comment_count
                        break
            
            # ã‚·ã‚§ã‚¢æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            share_patterns = [
                r'(\d+(?:\.\d+)?[KMB]?)\s*(?:shares?|ã‚·ã‚§ã‚¢)',
                r'ã‚·ã‚§ã‚¢[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
                r'share[s]?\s*[ï¼š:]\s*(\d+(?:\.\d+)?[KMB]?)',
            ]
            
            for pattern in share_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    share_count = self._parse_count_string(match.group(1))
                    if share_count and share_count > 0:
                        details['share_count'] = share_count
                        break
            
            # æŠ•ç¨¿æ—¥æ™‚ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            date_patterns = [
                r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
                r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY
                r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY
                r'(\d+)\s*(?:hours?|æ™‚é–“)\s*ago',  # X hours ago
                r'(\d+)\s*(?:days?|æ—¥)\s*ago',  # X days ago
                r'(\d+)\s*(?:weeks?|é€±é–“)\s*ago',  # X weeks ago
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    details['create_time_raw'] = match.group(1)
                    break
        
        except Exception as e:
            self.logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_e2e_attributes_enhanced(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """data-e2eå±æ€§ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            # çµ±è¨ˆæƒ…å ±ã®data-e2eå±æ€§ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            e2e_mappings = {
                'like-count': 'like_count',
                'comment-count': 'comment_count',
                'share-count': 'share_count',
                'video-views': 'view_count',
                'video-view-count': 'view_count',
                'browse-video-desc': 'description',
                'browse-username': 'author_username',
                'browse-nickname': 'author_display_name'
            }
            
            for e2e_value, detail_key in e2e_mappings.items():
                elements = soup.find_all(attrs={'data-e2e': e2e_value})
                for element in elements:
                    text = element.get_text(strip=True)
                    if text:
                        if detail_key in ['like_count', 'comment_count', 'share_count', 'view_count']:
                            count = self._parse_count_string(text)
                            if count is not None and count > 0:
                                details[detail_key] = count
                                break
                        else:
                            details[detail_key] = text
                            break
        
        except Exception as e:
            self.logger.warning(f"data-e2eå±æ€§æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_css_selectors(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """CSS ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹ç›´æ¥æŠ½å‡º"""
        details = {}
        
        try:
            # TikTokã®ä¸€èˆ¬çš„ãªCSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            css_selectors = {
                # çµ±è¨ˆæƒ…å ±
                '[data-e2e="like-count"]': 'like_count',
                '[data-e2e="comment-count"]': 'comment_count',
                '[data-e2e="share-count"]': 'share_count',
                '[data-e2e="video-views"]': 'view_count',
                
                # ä½œè€…æƒ…å ±
                '[data-e2e="browse-username"]': 'author_username',
                '[data-e2e="browse-nickname"]': 'author_display_name',
                
                # å‹•ç”»æƒ…å ±
                '[data-e2e="browse-video-desc"]': 'description',
                
                # ãã®ä»–ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
                '.video-meta-caption': 'description',
                '.author-uniqueId': 'author_username',
                '.author-nickname': 'author_display_name',
            }
            
            for selector, detail_key in css_selectors.items():
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        text = element.get_text(strip=True)
                        if text:
                            if detail_key in ['like_count', 'comment_count', 'share_count', 'view_count']:
                                count = self._parse_count_string(text)
                                if count is not None and count > 0:
                                    details[detail_key] = count
                                    break
                            else:
                                details[detail_key] = text
                                break
                except Exception:
                    continue
        
        except Exception as e:
            self.logger.warning(f"CSS ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _extract_from_regex_patterns(self, html_content: str) -> Dict[str, Any]:
        """æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹é«˜åº¦ãªæŠ½å‡º"""
        details = {}
        
        try:
            # å†ç”Ÿæ•°ã®é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            view_patterns = [
                r'"playCount":\s*(\d+)',
                r'"viewCount":\s*(\d+)',
                r'"play_count":\s*(\d+)',
                r'"view_count":\s*(\d+)',
                r'playCount["\']:\s*["\']?(\d+)',
                r'viewCount["\']:\s*["\']?(\d+)',
            ]
            
            for pattern in view_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    try:
                        view_count = int(match.group(1))
                        if view_count > 0:
                            details['view_count'] = view_count
                            break
                    except ValueError:
                        continue
            
            # ã„ã„ã­æ•°ã®é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            like_patterns = [
                r'"diggCount":\s*(\d+)',
                r'"likeCount":\s*(\d+)',
                r'"like_count":\s*(\d+)',
                r'diggCount["\']:\s*["\']?(\d+)',
                r'likeCount["\']:\s*["\']?(\d+)',
            ]
            
            for pattern in like_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    try:
                        like_count = int(match.group(1))
                        if like_count > 0:
                            details['like_count'] = like_count
                            break
                    except ValueError:
                        continue
            
            # æŠ•ç¨¿æ—¥æ™‚ã®é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            time_patterns = [
                r'"createTime":\s*(\d+)',
                r'"create_time":\s*(\d+)',
                r'"uploadDate":\s*["\']([^"\']+)["\']',
                r'"published_at":\s*["\']([^"\']+)["\']',
                r'createTime["\']:\s*["\']?(\d+)',
            ]
            
            for pattern in time_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    time_value = match.group(1)
                    try:
                        # Unix timestampã®å ´åˆ
                        if time_value.isdigit():
                            timestamp = int(time_value)
                            # TikTokã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯é€šå¸¸ç§’å˜ä½
                            if timestamp > 1000000000:  # 2001å¹´ä»¥é™
                                details['create_time'] = timestamp
                                break
                        else:
                            # ISOå½¢å¼ã®å ´åˆ
                            details['create_time_iso'] = time_value
                            break
                    except ValueError:
                        continue
            
            # ä½œè€…æƒ…å ±ã®é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            author_patterns = [
                r'"uniqueId":\s*["\']([^"\']+)["\']',
                r'"author":\s*["\']([^"\']+)["\']',
                r'"username":\s*["\']([^"\']+)["\']',
                r'uniqueId["\']:\s*["\']([^"\']+)["\']',
            ]
            
            for pattern in author_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    author = match.group(1)
                    if author and len(author) > 0:
                        details['author_username'] = author
                        break
        
        except Exception as e:
            self.logger.warning(f"æ­£è¦è¡¨ç¾æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_item_module_data_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ItemModuleãƒ‡ãƒ¼ã‚¿ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            if 'desc' in data:
                details['description'] = data['desc']
            if 'createTime' in data:
                details['create_time'] = data['createTime']
            
            # çµ±è¨ˆæƒ…å ±
            if 'stats' in data:
                stats = data['stats']
                if 'playCount' in stats:
                    details['view_count'] = stats['playCount']
                if 'diggCount' in stats:
                    details['like_count'] = stats['diggCount']
                if 'commentCount' in stats:
                    details['comment_count'] = stats['commentCount']
                if 'shareCount' in stats:
                    details['share_count'] = stats['shareCount']
            
            # ä½œè€…æƒ…å ±
            if 'author' in data:
                author = data['author']
                if 'uniqueId' in author:
                    details['author_username'] = author['uniqueId']
                if 'nickname' in author:
                    details['author_display_name'] = author['nickname']
                if 'verified' in author:
                    details['author_verified'] = author['verified']
            
            # å‹•ç”»æƒ…å ±
            if 'video' in data:
                video = data['video']
                if 'duration' in video:
                    details['duration'] = video['duration']
                if 'width' in video:
                    details['width'] = video['width']
                if 'height' in video:
                    details['height'] = video['height']
        
        except Exception as e:
            self.logger.warning(f"ItemModuleè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_user_module_data_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """UserModuleãƒ‡ãƒ¼ã‚¿ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            if 'uniqueId' in data:
                details['author_username'] = data['uniqueId']
            if 'nickname' in data:
                details['author_display_name'] = data['nickname']
            if 'verified' in data:
                details['author_verified'] = data['verified']
            if 'followerCount' in data:
                details['author_follower_count'] = data['followerCount']
            if 'followingCount' in data:
                details['author_following_count'] = data['followingCount']
            if 'videoCount' in data:
                details['author_video_count'] = data['videoCount']
        
        except Exception as e:
            self.logger.warning(f"UserModuleè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_video_detail_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """VideoDetailãƒ‡ãƒ¼ã‚¿ã‚’è§£æ"""
        details = {}
        
        try:
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, dict):
                        # å†å¸°çš„ã«è§£æ
                        sub_details = self._parse_video_detail_data(value)
                        details.update(sub_details)
        
        except Exception as e:
            self.logger.warning(f"VideoDetailè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_webapp_video_detail(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """webapp.video-detailãƒ‡ãƒ¼ã‚¿ã‚’è§£æ"""
        details = {}
        
        try:
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, dict):
                        # å†å¸°çš„ã«è§£æ
                        sub_details = self._parse_webapp_video_detail(value)
                        details.update(sub_details)
        
        except Exception as e:
            self.logger.warning(f"webapp.video-detailè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_video_object_enhanced(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """VideoObjectãƒ‡ãƒ¼ã‚¿ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            if 'name' in data:
                details['title'] = data['name']
            if 'description' in data:
                details['description'] = data['description']
            if 'uploadDate' in data:
                details['create_time_iso'] = data['uploadDate']
            if 'thumbnailUrl' in data:
                details['thumbnail_url'] = data['thumbnailUrl']
            if 'contentUrl' in data:
                details['video_url'] = data['contentUrl']
            if 'duration' in data:
                details['duration'] = data['duration']
            if 'width' in data:
                details['width'] = data['width']
            if 'height' in data:
                details['height'] = data['height']
        
        except Exception as e:
            self.logger.warning(f"VideoObjectè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_interaction_stats_enhanced(self, stats: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        details = {}
        
        try:
            for stat in stats:
                interaction_type = stat.get('interactionType', {}).get('@type', '')
                user_interaction_count = stat.get('userInteractionCount')
                
                if interaction_type == 'LikeAction' and user_interaction_count:
                    details['like_count'] = int(user_interaction_count)
                elif interaction_type == 'CommentAction' and user_interaction_count:
                    details['comment_count'] = int(user_interaction_count)
                elif interaction_type == 'ShareAction' and user_interaction_count:
                    details['share_count'] = int(user_interaction_count)
                elif interaction_type == 'WatchAction' and user_interaction_count:
                    details['view_count'] = int(user_interaction_count)
        
        except Exception as e:
            self.logger.warning(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return details
    
    def _parse_count_string(self, count_str: str) -> Optional[int]:
        """ã‚«ã‚¦ãƒ³ãƒˆæ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            count_str = count_str.upper().strip().replace(',', '').replace(' ', '')
            
            if count_str.endswith('K'):
                return int(float(count_str[:-1]) * 1000)
            elif count_str.endswith('M'):
                return int(float(count_str[:-1]) * 1000000)
            elif count_str.endswith('B'):
                return int(float(count_str[:-1]) * 1000000000)
            else:
                # æ•°å€¤ã®ã¿ã®å ´åˆ
                return int(float(count_str))
                
        except (ValueError, TypeError):
            return None
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            **self.stats,
            'success_rate': self.stats['successful_requests'] / max(self.stats['total_requests'], 1),
            'view_count_extraction_rate': self.stats['view_count_extracted'] / max(self.stats['videos_with_details'], 1),
            'create_time_extraction_rate': self.stats['create_time_extracted'] / max(self.stats['videos_with_details'], 1),
            'author_extraction_rate': self.stats['author_extracted'] / max(self.stats['videos_with_details'], 1)
        }


def test_enhanced_scraper():
    """æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    import os
    
    # APIã‚­ãƒ¼ã‚’å–å¾—
    api_key = os.getenv('SCRAPERAPI_KEY')
    if not api_key:
        print("âŒ SCRAPERAPI_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # ãƒ†ã‚¹ãƒˆç”¨å‹•ç”»URL
    test_urls = [
        "https://www.tiktok.com/@_quietlydope/video/7535094688726945079",
        "https://www.tiktok.com/@ohnoitsrolo/video/7534370912854985997",
    ]
    
    scraper = EnhancedVideoDetailScraper(api_key)
    
    print("ğŸ” æ”¹è‰¯ç‰ˆå‹•ç”»è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n=== ãƒ†ã‚¹ãƒˆ {i}: {url} ===")
        
        details = scraper.get_video_details(url)
        
        if details:
            print("âœ… è©³ç´°æƒ…å ±å–å¾—æˆåŠŸ")
            print(f"å‹•ç”»ID: {details.get('video_id', 'N/A')}")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {details.get('title', 'N/A')}")
            print(f"ä½œè€…: {details.get('author_username', 'N/A')}")
            print(f"å†ç”Ÿæ•°: {details.get('view_count', 'N/A')}")
            print(f"ã„ã„ã­æ•°: {details.get('like_count', 'N/A')}")
            print(f"æŠ•ç¨¿æ—¥æ™‚: {details.get('create_time', 'N/A')}")
            
            # è©³ç´°æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            import json
            os.makedirs('debug', exist_ok=True)
            with open(f'debug/enhanced_video_details_{details.get("video_id", i)}.json', 'w', encoding='utf-8') as f:
                json.dump(details, f, ensure_ascii=False, indent=2)
            print(f"è©³ç´°æƒ…å ±ã‚’ä¿å­˜: debug/enhanced_video_details_{details.get('video_id', i)}.json")
        else:
            print("âŒ è©³ç´°æƒ…å ±å–å¾—å¤±æ•—")
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    stats = scraper.get_stats()
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats['total_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"å†ç”Ÿæ•°æŠ½å‡ºç‡: {stats['view_count_extraction_rate']:.2%}")
    print(f"æŠ•ç¨¿æ—¥æ™‚æŠ½å‡ºç‡: {stats['create_time_extraction_rate']:.2%}")
    print(f"ä½œè€…æƒ…å ±æŠ½å‡ºç‡: {stats['author_extraction_rate']:.2%}")


if __name__ == "__main__":
    test_enhanced_scraper()

